# scripts/train_lora_minimal.py
# CPU í™˜ê²½ì—ì„œ ì ì€ ë°ì´í„°ë¡œ LoRA í•™ìŠµ
from peft import LoraConfig, get_peft_model
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForSeq2Seq
from datasets import load_dataset
import torch
import os

print("ğŸš€ LoRA ìµœì†Œ í•™ìŠµ ì‹œì‘ (CPU í™˜ê²½)")

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
model_name = "EleutherAI/polyglot-ko-1.3b"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# íŒ¨ë”© í† í° ì„¤ì •
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" 

print("ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...")
# CPU í™˜ê²½ì— ë§ëŠ” ì„¤ì •
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32,  # CPUì—ì„œëŠ” float32
    device_map=None  # CPU ì‚¬ìš©
)

# LoRA ì„¤ì • (ìµœì†Œ ì„¤ì •)
lora_config = LoraConfig(
    r=4,  # ë­í¬ë¥¼ ë” ì‘ê²Œ ì„¤ì •
    lora_alpha=16,  # ì•ŒíŒŒê°’ë„ ì¤„ì„
    target_modules=["q_proj", "v_proj"],  # ë” ì ì€ ëª¨ë“ˆë§Œ í•™ìŠµ
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
print("âœ… LoRA ëª¨ë¸ ì„¤ì • ì™„ë£Œ")

# í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
print("\n--- í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ ---")
model.print_trainable_parameters()
print("---------------------------\n")

# ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
print("ğŸ“Š ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
dataset = load_dataset("json", data_files="../data/user_diaries.jsonl")["train"]
print(f"ğŸ“ˆ ì´ {len(dataset)}ê°œ ë°ì´í„° ë¡œë“œë¨")

# ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(examples):
    tokenized_input = tokenizer(
        examples["text"],
        max_length=512,  # ë” ì§§ê²Œ ì„¤ì •
        truncation=True,
        padding="max_length" 
    )
    tokenized_input["labels"] = tokenized_input["input_ids"].copy() 
    return tokenized_input

tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=["text"])
print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")

# ìµœì†Œ í•™ìŠµ ì¸ì ì„¤ì •
training_args = TrainingArguments(
    output_dir="./models/lora_minimal",
    per_device_train_batch_size=1,  # ë°°ì¹˜ í¬ê¸° 1
    gradient_accumulation_steps=4,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
    num_train_epochs=1,  # 1 ì—í¬í¬ë§Œ
    learning_rate=1e-4,  # ë‚®ì€ í•™ìŠµë¥ 
    logging_steps=5,  # ë” ìì£¼ ë¡œê¹…
    save_steps=25,  # ë” ìì£¼ ì €ì¥
    save_total_limit=1,
    fp16=False,  # CPUì—ì„œëŠ” fp16 ë¹„í™œì„±í™”
    remove_unused_columns=False,
    warmup_steps=10,  # ì›Œë°ì—… ì¶”ê°€
)

print("ğŸ¯ í•™ìŠµ ì‹œì‘...")
# Trainer ì´ˆê¸°í™” ë° í•™ìŠµ ì‹œì‘
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model, padding="longest", label_pad_token_id=tokenizer.pad_token_id),
)

trainer.train()

print("\nğŸ‰ LoRA ìµœì†Œ í•™ìŠµ ì™„ë£Œ!")
print("ğŸ“ ëª¨ë¸ì´ ./models/lora_minimal ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("ğŸ’¡ ì´ì œ generate_diary.pyì—ì„œ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.") 
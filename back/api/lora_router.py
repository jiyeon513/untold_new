from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import os
from typing import Optional
import asyncio
from db.connect import supabase

router = APIRouter()

model = None
tokenizer = None
is_model_loaded = False

class DiaryGenerationRequest(BaseModel):
    original_text: str
    user_id: str
    diary_id: str

class DiaryGenerationResponse(BaseModel):
    generated_text: str
    original_length: int
    generated_length: int
    model_version: str

async def load_lora_model():
    global model, tokenizer, is_model_loaded
    if is_model_loaded and model is not None and tokenizer is not None:
        print("ğŸ¤– LoRA ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    try:
        print("ğŸ¤– LoRA ëª¨ë¸ ë¡œë”© ì‹œì‘...")
        base_model_name = "EleutherAI/polyglot-ko-1.3b"
        lora_model_path = os.path.join(os.path.dirname(__file__), "..", "..", "lora", "models")

        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        tokenizer.padding_side = "left"

        compute_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=compute_dtype,
            device_map="auto" if torch.cuda.is_available() else None
        )
        print("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

        try:
            model = PeftModel.from_pretrained(base_model, lora_model_path)
            print("âœ… LoRA ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        except Exception as e:
            print(f"âŒ LoRA ê°€ì¤‘ì¹˜ ë¡œë”© ì‹¤íŒ¨: {e}")
            model = base_model
            is_model_loaded = False
            return

        model.eval()
        is_model_loaded = True
        print("âœ… LoRA ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì „ì²´ ë¡œë”© ì‹¤íŒ¨: {e}")
        model = None
        tokenizer = None
        is_model_loaded = False

@router.post("/lora/generate", response_model=DiaryGenerationResponse)
async def generate_diary_with_lora(request: DiaryGenerationRequest):
    try:
        print(f"ğŸ¤– í…ìŠ¤íŠ¸ ìƒì„± ìš”ì²­: user={request.user_id}, length={len(request.original_text)}")
        if not is_model_loaded or model is None or tokenizer is None:
            await load_lora_model()
            if not is_model_loaded:
                fallback_text = request.original_text + "\nì˜¤ëŠ˜ í•˜ë£¨ë„ í–‰ë³µí•œ í•˜ë£¨ì˜€ë‹¤."
                return DiaryGenerationResponse(
                    generated_text=fallback_text,
                    original_length=len(request.original_text),
                    generated_length=len(fallback_text),
                    model_version="fallback_model_not_loaded"
                )

        generated_text = await generate_personalized_text(request.original_text)
        print(f"âœ… ìƒì„± ì™„ë£Œ: {generated_text}")
        return DiaryGenerationResponse(
            generated_text=generated_text,
            original_length=len(request.original_text),
            generated_length=len(generated_text),
            model_version="lora-v1.0"
        )

    except Exception as e:
        print(f"âŒ í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        fallback_text = request.original_text + "\nì˜¤ëŠ˜ í•˜ë£¨ë„ í–‰ë³µí•œ í•˜ë£¨ì˜€ë‹¤."
        return DiaryGenerationResponse(
            generated_text=fallback_text,
            original_length=len(request.original_text),
            generated_length=len(fallback_text),
            model_version="fallback_exception"
        )

async def generate_personalized_text(original_text: str) -> str:
    try:
        if model is None or tokenizer is None:
            return original_text + "\ní–‰ë³µí•œ í•˜ë£¨ì˜€ë‹¤."

        prompt = f"""ë‹¤ìŒì€ ì¼ê¸°ë¥¼ ê°ì„±ì ì¸ ë¬¸ì²´ë¡œ ê°œì„ í•˜ëŠ” ì˜ˆì‹œì•¼:

ì…ë ¥: ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì¢‹ì•˜ë‹¤.
ì¶œë ¥: í–‡ì‚´ì´ ë¹„ì¶”ëŠ” ì•„ì¹¨, ë§ˆìŒê¹Œì§€ ë”°ëœ»í•´ì§€ëŠ” í•˜ë£¨ì˜ ì‹œì‘ì´ì—ˆë‹¤.

ì…ë ¥: {original_text}
ì¶œë ¥:"""

        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512,
            padding=True
        )
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            generated_ids = model.generate(
                inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                do_sample=True,
                top_k=50,
                top_p=0.95,
                temperature=0.8,
                max_new_tokens=200,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=3,
                repetition_penalty=1.15,
                length_penalty=1.2,
                early_stopping=True
            )

        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

        # ì¶œë ¥ íŒŒì‹±
        idx = generated_text.rfind("ì¶œë ¥:")
        if idx != -1:
            generated_text = generated_text[idx + len("ì¶œë ¥:"):].strip()
        else:
            if original_text in generated_text:
                generated_text = generated_text.split(original_text)[-1].strip()

        # ê¸ì • ë¬¸ì¥ í•˜ë“œì½”ë”©
        generated_text = post_process_text(generated_text, original_text)
        return generated_text + "\ní–‰ë³µí•œ í•˜ë£¨ì˜€ë‹¤."

    except Exception as e:
        print(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")
        return original_text + "\ní–‰ë³µí•œ í•˜ë£¨ì˜€ë‹¤."

def post_process_text(generated_text: str, original_text: str) -> str:
    try:
        text = generated_text.strip()

        lines = text.split('\n')
        cleaned_lines = []
        seen = set()
        for line in lines:
            line = line.strip()
            if line and line not in seen:
                cleaned_lines.append(line)
                seen.add(line)
        text = '\n'.join(cleaned_lines)


        if len(text) < 10:  # ìµœì†Œ 10ì ì´ìƒì´ë©´ ì‚¬ìš©
            print(f"âš ï¸ ìƒì„±ëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: {len(text)}ì. ì›ë³¸ ë°˜í™˜.")
            return original_text

        return text
    except Exception as e:
        print(f"âŒ í›„ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return generated_text

@router.on_event("startup")
async def startup_event():
    print("ğŸš€ ì„œë²„ ì‹œì‘: ëª¨ë¸ ë¡œë“œ ì‹œë„")
    try:
        await load_lora_model()
    except Exception as e:
        print(f"âš ï¸ ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")

# back/rl_core/rl_env.py

import numpy as np
from typing import List, Dict, Any, Tuple, Optional
import math # ì˜¬ë¦¼ ì—°ì‚°ì„ ìœ„í•´ import
import torch # torch.Tensor íƒ€ì…ì„ ìœ„í•´ ì„í¬íŠ¸ (PPOModelê³¼ ì—°ë™)

class RLConfig:
    """ê°•í™”í•™ìŠµ í™˜ê²½ì˜ ì„¤ì •ì„ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤"""
    MAX_ROWS = 3 # ì¼ê¸° ë ˆì´ì•„ì›ƒì˜ ìµœëŒ€ í–‰ ìˆ˜ (ê·¸ë¦¬ë“œ í¬ê¸°)
    MAX_COLS = 4 # ì¼ê¸° ë ˆì´ì•„ì›ƒì˜ ìµœëŒ€ ì—´ ìˆ˜ (ê·¸ë¦¬ë“œ í¬ê¸°)
    
    # ë³´ìƒ ê°’
    REWARD_SAVE = 100.0        # ì¼ê¸° ì €ì¥ ì‹œ ë³´ìƒ (ê¸ì •ì )
    REWARD_MODIFY = -50.0      # ë ˆì´ì•„ì›ƒ ìˆ˜ì • ì‹œ ë³´ìƒ (ìˆ˜ê³ ë¡œì›€, AI ì¶”ì²œ ë¶ˆë§Œì¡±ìœ¼ë¡œ í•´ì„)
    REWARD_REGENERATE = -100.0 # ë ˆì´ì•„ì›ƒ ì¬ì¶”ì²œ ìš”ì²­ ì‹œ ë³´ìƒ (ë¶ˆë§Œì¡± ì‹¬í™”)
    REWARD_INVALID_ACTION = -1000.0 # ìœ íš¨í•˜ì§€ ì•Šì€ í–‰ë™ (ì˜ˆ: ì´ë¯¸ ì ìœ ëœ ì¹¸ì— ë°°ì¹˜)

    # ìƒíƒœ ë²¡í„° ì°¨ì› ê´€ë ¨ ì„¤ì •
    MAX_CARDS_IN_LAYOUT = MAX_ROWS * MAX_COLS # ë ˆì´ì•„ì›ƒì— ë“¤ì–´ê°ˆ ìˆ˜ ìˆëŠ” ìµœëŒ€ ì¹´ë“œ ê°œìˆ˜ (ìµœëŒ€ 12ê°œ)

    # ì¹´ë“œ íŠ¹ì§• ì •ì˜
    # í˜„ì¬: has_image(1), has_content(1)
    NUM_CARD_FEATURES = 2 

    # ì‚¬ìš©ì íŠ¹ì§• ì •ì˜
    # í˜„ì¬: average_satisfaction(1), total_diaries(1)
    NUM_USER_FEATURES = 2 

    # ìƒíƒœ ë²¡í„°ì˜ ìµœì¢… ì°¨ì› ê³„ì‚°
    # ì‚¬ìš©ì íŠ¹ì§• + í˜„ì¬ ë°°ì¹˜í•˜ë ¤ëŠ” ì¹´ë“œ ì¸ë±ìŠ¤ + ì„ íƒëœ ì¹´ë“œ ê°œìˆ˜ + (ìµœëŒ€ ì¹´ë“œ ê°œìˆ˜ * ì¹´ë“œë‹¹ íŠ¹ì§•) + ê·¸ë¦¬ë“œ ìƒíƒœ (ê° ì¹¸ì˜ ì ìœ  ì—¬ë¶€)
    STATE_DIM = NUM_USER_FEATURES + \
                1 + \
                1 + \
                (MAX_CARDS_IN_LAYOUT * NUM_CARD_FEATURES) + \
                (MAX_ROWS * MAX_COLS) # ê·¸ë¦¬ë“œ ì¹¸ ìˆ˜ (0: ë¹„ì–´ìˆìŒ, 1: ì ìœ ë¨)

    # í–‰ë™ ê³µê°„ì˜ í¬ê¸° (Discrete Action Space: ê° ê·¸ë¦¬ë“œ ì…€ì˜ ì¸ë±ìŠ¤)
    # 0ë¶€í„° (MAX_ROWS * MAX_COLS - 1)ê¹Œì§€ì˜ ì •ìˆ˜ (ì˜ˆ: 0~11)
    ACTION_SPACE_SIZE = MAX_ROWS * MAX_COLS


class RLEnvironment:
    """
    ì¼ê¸° UI ë°°ì¹˜ ê°•í™”ë¥¼ ìœ„í•œ í™˜ê²½ í´ë˜ìŠ¤
    ì—ì´ì „íŠ¸ê°€ ìƒí˜¸ì‘ìš©í•  í™˜ê²½ì„ ì •ì˜í•©ë‹ˆë‹¤.
    """
    def __init__(self, config: RLConfig):
        self.config = config
        self.current_user_id: str = None
        self.selected_cards_data: List[Dict[str, Any]] = [] # ì¹´ë“œ IDê°€ ì•„ë‹Œ, ì‹¤ì œ ì¹´ë“œ ìƒì„¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
        self.num_selected_cards: int = 0
        self.user_profile_data: Dict[str, Any] = {}
        self.grid_state: np.ndarray = np.zeros((self.config.MAX_ROWS, self.config.MAX_COLS), dtype=np.int32) # ê·¸ë¦¬ë“œ ì ìœ  ìƒíƒœ (0: ë¹„ì–´ìˆìŒ, 1: ì ìœ ë¨)
        self.current_card_index_to_place: int = 0 # í˜„ì¬ ë°°ì¹˜í•˜ë ¤ëŠ” ì¹´ë“œ ì¸ë±ìŠ¤ (selected_cards_data ë¦¬ìŠ¤íŠ¸ ë‚´ ì¸ë±ìŠ¤)

    def reset(self, 
              user_id: str, 
              selected_card_ids: List[str], 
              all_cards_data_raw: Dict[str, Dict[str, Any]], 
              user_profile_data_raw: Dict[str, Any]) -> np.ndarray:
        """
        í™˜ê²½ì„ ì´ˆê¸°í™”í•˜ê³  ì´ˆê¸° ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        Args:
            user_id (str): í˜„ì¬ ì¼ê¸°ë¥¼ ìƒì„±í•˜ë ¤ëŠ” ì‚¬ìš©ì ID.
            selected_card_ids (List[str]): ì‚¬ìš©ìê°€ ê¸€ê°ìœ¼ë¡œ ì„ íƒí•œ ì¹´ë“œ ID (ë¬¸ìì—´) ë¦¬ìŠ¤íŠ¸.
            all_cards_data_raw (Dict[str, Dict[str, Any]]): ê° ì¹´ë“œ IDì— í•´ë‹¹í•˜ëŠ” ìƒì„¸ ë°ì´í„° (Dict[card_id_str, card_data_dict]).
            user_profile_data_raw (Dict[str, Any]): ì‚¬ìš©ì í”„ë¡œí•„ ë°ì´í„° (dict í˜•íƒœ).
        Returns:
            np.ndarray: ê°•í™”í•™ìŠµ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ì´ˆê¸° ìƒíƒœ ë²¡í„°.
        """
        self.current_user_id = user_id
        # selected_card_idsì— í•´ë‹¹í•˜ëŠ” ì¹´ë“œë“¤ë§Œ í•„í„°ë§í•˜ì—¬ ì €ì¥
        self.selected_cards_data = [all_cards_data_raw[cid] for cid in selected_card_ids if cid in all_cards_data_raw]
        self.num_selected_cards = len(self.selected_cards_data)
        self.user_profile_data = user_profile_data_raw
        self.grid_state = np.zeros((self.config.MAX_ROWS, self.config.MAX_COLS), dtype=np.int32) # ê·¸ë¦¬ë“œ ì ìœ  ìƒíƒœ ì´ˆê¸°í™” (ëª¨ë‘ ë¹„ì–´ìˆìŒ)
        self.current_card_index_to_place = 0 # ì²« ë²ˆì§¸ ì¹´ë“œë¶€í„° ë°°ì¹˜ ì‹œì‘

        # print(f"RLEnvironment reset for user {user_id} with {self.num_selected_cards} cards.")
        return self._get_state_vector()

    def _get_state_vector(self) -> np.ndarray:
        """
        ê°•í™”í•™ìŠµ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ìƒíƒœ ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        ì‚¬ìš©ì í”„ë¡œí•„ ë°ì´í„°, í˜„ì¬ ë°°ì¹˜í•˜ë ¤ëŠ” ì¹´ë“œ ì¸ë±ìŠ¤, ì„ íƒëœ ì¹´ë“œ ë°ì´í„°, í˜„ì¬ ê·¸ë¦¬ë“œ ìƒíƒœë¥¼ í™œìš©í•©ë‹ˆë‹¤.
        """
        # 1. ì‚¬ìš©ì íŠ¹ì§•
        user_features = np.array([
            self.user_profile_data.get('average_satisfaction', 0.5), 
            self.user_profile_data.get('total_diaries', 1.0) / 100.0 # ìŠ¤ì¼€ì¼ë§ (ìµœëŒ€ 100ê°œ ì¼ê¸° ê°€ì •)
        ])
        
        # 2. í˜„ì¬ ë°°ì¹˜í•˜ë ¤ëŠ” ì¹´ë“œ ì¸ë±ìŠ¤ (ì •ê·œí™”)
        current_card_idx_feature = np.array([self.current_card_index_to_place / self.config.MAX_CARDS_IN_LAYOUT])

        # 3. ì„ íƒëœ ì¹´ë“œ ê°œìˆ˜ (ì •ê·œí™”)
        num_cards_feature = np.array([self.num_selected_cards / self.config.MAX_CARDS_IN_LAYOUT])

        # 4. ê° ì¹´ë“œì˜ íŠ¹ì§• (íŒ¨ë”©/íŠ¸ë ì¼€ì´ì…˜ í¬í•¨)
        card_features_list = []
        for card in self.selected_cards_data:
            has_image = 1.0 if card.get('image_url') else 0.0
            has_content = 1.0 if card.get('content') else 0.0
            # TODO: ì¹´í…Œê³ ë¦¬ (source_type, category) ì›-í•« ì¸ì½”ë”© ë“± ë” ë§ì€ íŠ¹ì§• ì¶”ê°€ ê°€ëŠ¥
            card_features_list.append([has_image, has_content])
        
        # ì„ íƒëœ ì¹´ë“œ íŠ¹ì§•ë“¤ì„ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í‰íƒ„í™”. MAX_CARDS_IN_LAYOUTì— ë§ì¶° íŒ¨ë”©/íŠ¸ë ì¼€ì´ì…˜.
        flat_card_features = np.array(card_features_list).flatten()
        
        expected_card_features_dim = self.config.MAX_CARDS_IN_LAYOUT * self.config.NUM_CARD_FEATURES
        if len(flat_card_features) < expected_card_features_dim:
            # ë¶€ì¡±í•˜ë©´ 0ìœ¼ë¡œ íŒ¨ë”©
            card_features_padded = np.pad(flat_card_features, (0, expected_card_features_dim - len(flat_card_features)), 'constant')
        elif len(flat_card_features) > expected_card_features_dim:
            # ë§ìœ¼ë©´ ì˜ë¼ëƒ„
            card_features_padded = flat_card_features[:expected_card_features_dim]
        else:
            card_features_padded = flat_card_features

        # 5. ê·¸ë¦¬ë“œ ìƒíƒœ (ì–´ë–¤ ì¹¸ì´ ì ìœ ë˜ì—ˆëŠ”ì§€)
        grid_flat = self.grid_state.flatten().astype(np.float32) # PPO ëª¨ë¸ ì…ë ¥ì„ ìœ„í•´ float32ë¡œ ë³€í™˜

        # ëª¨ë“  íŠ¹ì§• ë²¡í„°ë¥¼ ì—°ê²°í•˜ì—¬ ìµœì¢… ìƒíƒœ ë²¡í„° ìƒì„±
        state_vector = np.concatenate([
            user_features, 
            current_card_idx_feature, 
            num_cards_feature, 
            card_features_padded, 
            grid_flat
        ]).astype(np.float32) # ìµœì¢…ì ìœ¼ë¡œ float32ë¡œ ë³€í™˜

        # ì •ì˜ëœ STATE_DIMê³¼ ì‹¤ì œ ìƒì„±ëœ ë²¡í„°ì˜ í¬ê¸°ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ (ë””ë²„ê¹…ìš©)
        if state_vector.shape[0] != self.config.STATE_DIM:
            print(f"Warning: State vector dimension mismatch! Expected {self.config.STATE_DIM}, got {state_vector.shape[0]}.")
            # í•„ìš”ì— ë”°ë¼ íŒ¨ë”©/íŠ¸ë ì¼€ì´ì…˜ ë¡œì§ ì¶”ê°€
            if state_vector.shape[0] < self.config.STATE_DIM:
                state_vector = np.pad(state_vector, (0, self.config.STATE_DIM - state_vector.shape[0]), 'constant')
            else:
                state_vector = state_vector[:self.config.STATE_DIM]

        return state_vector

    def step(self, action_idx: int) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:
        """
        ì—ì´ì „íŠ¸ì˜ í–‰ë™(ê·¸ë¦¬ë“œ ì…€ ì„ íƒ)ì„ ì‹¤í–‰í•˜ê³  ë‹¤ìŒ ìƒíƒœ, ë³´ìƒ, ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€, ì¶”ê°€ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        Args:
            action_idx (int): ëª¨ë¸ì´ ì„ íƒí•œ ê·¸ë¦¬ë“œ ì…€ì˜ ì¸ë±ìŠ¤ (0 ~ MAX_ROWS*MAX_COLS - 1).
        Returns:
            Tuple[np.ndarray, float, bool, Dict[str, Any]]:
                - next_state: ë‹¤ìŒ ìƒíƒœ ë²¡í„°.
                - reward: ì´ í–‰ë™ìœ¼ë¡œ ì–»ì€ ë³´ìƒ (ë‹¨ì¼ ìŠ¤í… ë³´ìƒ).
                - done: ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€ (ëª¨ë“  ì¹´ë“œê°€ ë°°ì¹˜ë˜ì—ˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ í–‰ë™).
                - info: ì¶”ê°€ ì •ë³´ (í˜„ì¬ ë°°ì¹˜ëœ ì¹´ë“œ ì •ë³´ ë“±).
        """
        reward = 0.0
        done = False
        info = {"placed_card_id": None, "row": -1, "col": -1}

        # action_idxë¥¼ (row, col)ìœ¼ë¡œ ë³€í™˜
        row = action_idx // self.config.MAX_COLS
        col = action_idx % self.config.MAX_COLS

        # 1. ìœ íš¨í•˜ì§€ ì•Šì€ í–‰ë™ ì²˜ë¦¬ (ì´ë¯¸ ì ìœ ëœ ì¹¸ì— ë°°ì¹˜ ì‹œë„)
        if not (0 <= row < self.config.MAX_ROWS and 0 <= col < self.config.MAX_COLS):
            reward = self.config.REWARD_INVALID_ACTION # ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ í–‰ë™
            done = True
            print(f"Invalid action: row={row}, col={col} out of bounds.")
        elif self.grid_state[row, col] == 1:
            reward = self.config.REWARD_INVALID_ACTION # ì´ë¯¸ ì ìœ ëœ ì¹¸
            done = True
            print(f"Invalid action: Cell ({row}, {col}) already occupied.")
        else:
            # 2. ìœ íš¨í•œ í–‰ë™: í˜„ì¬ ì¹´ë“œë¥¼ í•´ë‹¹ ìœ„ì¹˜ì— ë°°ì¹˜
            current_card_data = self.selected_cards_data[self.current_card_index_to_place]
            card_id = current_card_data['id']
            
            self.grid_state[row, col] = 1 # í•´ë‹¹ ì¹¸ì„ ì ìœ ë¨ìœ¼ë¡œ í‘œì‹œ

            info["placed_card_id"] = card_id
            info["row"] = row
            info["col"] = col
            info["order_index"] = self.current_card_index_to_place

            # 3. ë‹¤ìŒ ì¹´ë“œ ì¸ë±ìŠ¤ë¡œ ì´ë™
            self.current_card_index_to_place += 1

            # 4. ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            if self.current_card_index_to_place >= self.num_selected_cards:
                done = True # ëª¨ë“  ì¹´ë“œë¥¼ ë°°ì¹˜í–ˆìœ¼ë©´ ì—í”¼ì†Œë“œ ì¢…ë£Œ
                # ì´ ìŠ¤í…ì—ì„œëŠ” ë³´ìƒì„ 0ìœ¼ë¡œ ë‘ê³ , ìµœì¢… ë³´ìƒì€ episode_finish_rewardì—ì„œ ì²˜ë¦¬.
                # ë˜ëŠ” ê° ìŠ¤í…ë§ˆë‹¤ ì‘ì€ ë³´ìƒì„ ì¤„ ìˆ˜ë„ ìˆìŒ (ì˜ˆ: ì¹´ë“œ ë°°ì¹˜ ì„±ê³µ ë³´ìƒ)
                reward = 1.0 # ì¹´ë“œ í•˜ë‚˜ ì„±ê³µì ìœ¼ë¡œ ë°°ì¹˜ì— ëŒ€í•œ ì‘ì€ ê¸ì • ë³´ìƒ

        next_state = self._get_state_vector()
        return next_state, reward, done, info

    def calculate_reward(self, feedback_type: str, details: Dict[str, Any] = None) -> float:
        """
        ì‚¬ìš©ì í”¼ë“œë°±ì— ë”°ë¼ ìµœì¢… ë³´ìƒ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        ë ˆì´ì•„ì›ƒ ì°¨ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì •êµí•œ ë³´ìƒ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        if feedback_type == 'save':
            # ê¸°ë³¸ ì €ì¥ ë³´ìƒ
            base_reward = self.config.REWARD_SAVE
            
            # ë ˆì´ì•„ì›ƒ ì°¨ì´ ê¸°ë°˜ ë³´ìƒ ì¡°ì •
            if details and 'layout_difference' in details:
                layout_diff = details['layout_difference']
                layout_reward = details.get('layout_reward', 0)
                
                # ë ˆì´ì•„ì›ƒ ì°¨ì´ê°€ ìˆìœ¼ë©´ ë³´ìƒ ì¡°ì •
                if layout_diff > 0:
                    # ì°¨ì´ê°€ í´ìˆ˜ë¡ ë³´ìƒ ê°ì†Œ (ì‚¬ìš©ìê°€ ë§ì´ ìˆ˜ì •í–ˆë‹¤ëŠ” ì˜ë¯¸)
                    adjusted_reward = base_reward + layout_reward
                    print(f"ğŸ¯ ë ˆì´ì•„ì›ƒ ì°¨ì´ ê¸°ë°˜ ë³´ìƒ ì¡°ì •: ê¸°ë³¸={base_reward}, ë ˆì´ì•„ì›ƒ={layout_reward}, ìµœì¢…={adjusted_reward}")
                    return adjusted_reward
                else:
                    # AI ì¶”ì²œì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œ ê²½ìš° ì¶”ê°€ ë³´ìƒ
                    bonus_reward = 20
                    total_reward = base_reward + bonus_reward
                    print(f"ğŸ¯ AI ì¶”ì²œ ì‚¬ìš© ë³´ë„ˆìŠ¤: ê¸°ë³¸={base_reward}, ë³´ë„ˆìŠ¤={bonus_reward}, ìµœì¢…={total_reward}")
                    return total_reward
            
            return base_reward
            
        elif feedback_type == 'modify':
            # ìˆ˜ì • ë³´ìƒ (ë ˆì´ì•„ì›ƒ ì°¨ì´ ê³ ë ¤)
            base_reward = self.config.REWARD_MODIFY
            
            if details and 'layout_difference' in details:
                layout_diff = details['layout_difference']
                # ì°¨ì´ê°€ í´ìˆ˜ë¡ ë” í° ë¶€ì •ì  ë³´ìƒ
                layout_penalty = -layout_diff * 10
                total_reward = base_reward + layout_penalty
                print(f"ğŸ¯ ìˆ˜ì • ë³´ìƒ: ê¸°ë³¸={base_reward}, ë ˆì´ì•„ì›ƒ í˜ë„í‹°={layout_penalty}, ìµœì¢…={total_reward}")
                return total_reward
            
            return base_reward
            
        elif feedback_type == 'regenerate':
            # ì¬ìƒì„± ìš”ì²­ (ê°€ì¥ ë¶€ì •ì )
            return self.config.REWARD_REGENERATE
        else:
            return 0.0  # ì•Œ ìˆ˜ ì—†ëŠ” í”¼ë“œë°±